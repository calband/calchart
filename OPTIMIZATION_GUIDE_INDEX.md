# CalChart Sanity Tester Performance Optimization - Complete Guide Index

## ğŸ“‹ Overview

The CalChart sanity validation tests are slow, especially in debug mode. This guide provides a **data-driven, phased approach** to identify and fix the bottlenecks.

**Key insight**: Don't guess where time is spentâ€”measure first, then optimize strategically.

---

## ğŸš€ Quick Start (TL;DR)

**If you have 2 minutes:**
â†’ Read [QUICK_REFERENCE.md](QUICK_REFERENCE.md)

**If you have 5 minutes:**
â†’ Read [PERFORMANCE_OPTIMIZATION_SUMMARY.md](PERFORMANCE_OPTIMIZATION_SUMMARY.md)

**If you have 30 minutes:**
â†’ Read [VISUAL_WORKFLOW.md](VISUAL_WORKFLOW.md)

---

## ğŸ“š Complete Documentation

### 1. [QUICK_REFERENCE.md](QUICK_REFERENCE.md) â€” The 5-Minute Overview
**What it covers:**
- Problem statement (tests are slow)
- Solution approach (3 phases)
- Quick decision tree
- FAQ

**Best for**: Getting oriented quickly, understanding the approach

---

### 2. [PERFORMANCE_OPTIMIZATION_SUMMARY.md](PERFORMANCE_OPTIMIZATION_SUMMARY.md) â€” Executive Summary
**What it covers:**
- Problem statement with context
- Three-phase solution overview
- Timeline and effort estimates
- Success metrics
- Contact information

**Best for**: Planning the work, estimating time/effort

---

### 3. [VISUAL_WORKFLOW.md](VISUAL_WORKFLOW.md) â€” Visual Guides & Diagrams
**What it covers:**
- Decision tree flowchart
- Phase-by-phase visual comparisons
- Before/after timelines
- Testing strategy diagrams
- Code change summaries

**Best for**: Understanding the workflow visually, discussing with team

---

### 4. [PERFORMANCE_IMPROVEMENT_PLAN.md](PERFORMANCE_IMPROVEMENT_PLAN.md) â€” Detailed Strategy
**What it covers:**
- Current state analysis
- Four phases: Instrumentation, Analysis, and three optimization options
- Detailed implementation approach for each phase
- Integration points and dependencies
- Files to modify and examples

**Best for**: Technical deep dive, planning implementation

---

### 5. [PHASE_1_IMPLEMENTATION.md](PHASE_1_IMPLEMENTATION.md) â€” Step-by-Step Code Changes
**What it covers:**
- Exact Python code to add to `sanity_tester.py`
- Metrics data structures (dataclasses)
- Timing instrumentation code
- Report generation functions
- Testing instructions

**Best for**: Implementing Phase 1 (instrumentation)

---

## ğŸ¯ Three-Phase Approach

```
Phase 1: MEASURE
â†“ Add timing instrumentation (1-2 hours)
â†“ Identify actual bottleneck

Phase 2: ANALYZE
â†“ Run instrumented tests (30 min)
â†“ Determine which phase to optimize

Phase 3: OPTIMIZE
â†“ Choose from Phase 3A, 3B, or 3C based on findings
â†“ Implement, test, measure improvement
â†“ Repeat Phase 3 if needed for additional gains
```

---

## ğŸ“ˆ Optimization Options

After Phase 1 & 2, choose from:

| Option | If... | Impact | Effort | File(s) |
|--------|-------|--------|--------|---------|
| **Phase 3A** | calchart_cmd execution is 80%+ of time | 2-4x | Medium | `tools/calchart_cmd/` |
| **Phase 3B** | File comparison is 20%+ of time | 2-3x | Low | `resources/tests/` |
| **Phase 3C** | Python overhead is 15%+ of time | 1.2-1.5x | Low | `resources/tests/` |

---

## ğŸ“‹ Implementation Checklist

```
Phase 1: Instrumentation
 â˜ Read PHASE_1_IMPLEMENTATION.md
 â˜ Add timing imports to sanity_tester.py
 â˜ Create metrics data structures
 â˜ Add TimedBlock context manager
 â˜ Instrument run_command() and run_command_ps()
 â˜ Instrument comparison functions
 â˜ Add report generation
 â˜ Test instrumentation
 â˜ Commit: "Add performance instrumentation"

Phase 2: Analysis
 â˜ Run: python3 resources/tests/sanity_tester.py -d shows ...
 â˜ Note: total time and phase breakdown
 â˜ Identify bottleneck (exec%, comp%, overhead%)
 â˜ Choose Phase 3A/3B/3C based on findings

Phase 3A (if execution is bottleneck):
 â˜ Read PERFORMANCE_IMPROVEMENT_PLAN.md section "Phase 3A"
 â˜ Profile calchart_cmd execution
 â˜ Implement C++20 threading in calchart_cmd
 â˜ Rebuild and test
 â˜ Re-run Phase 1 to measure improvement
 â˜ Commit: "Phase 3A: Multi-thread calchart_cmd"

Phase 3B (if comparison is bottleneck):
 â˜ Read PERFORMANCE_IMPROVEMENT_PLAN.md section "Phase 3B"
 â˜ Pre-compile regex patterns
 â˜ Add fast-path checks
 â˜ Optimize extractValues() and compare_files()
 â˜ Re-run Phase 1 to measure improvement
 â˜ Commit: "Phase 3B: Optimize string comparison"

Phase 3C (if overhead is bottleneck):
 â˜ Read PERFORMANCE_IMPROVEMENT_PLAN.md section "Phase 3C"
 â˜ Replace manual Process with ProcessPoolExecutor
 â˜ Optimize batch sizes and pool configuration
 â˜ Re-run Phase 1 to measure improvement
 â˜ Commit: "Phase 3C: Optimize test harness"

Validation:
 â˜ All tests pass
 â˜ Metrics show 2-3x speedup in debug mode
 â˜ Metrics show 1.5-2x speedup in release mode
 â˜ Re-run full test suite to ensure correctness
```

---

## ğŸ” Key Files Modified

### Phase 1 (Always Required)
- **`resources/tests/sanity_tester.py`** â† Add instrumentation

### Phase 3A (If calchart_cmd is bottleneck)
- **`tools/calchart_cmd/main.cpp`** â† Add threading
- **`tools/calchart_cmd/calchart_cmd_parse.hpp`** â† Parallelize execution
- **`src/core/*`** â† Potentially add parallelization points

### Phase 3B (If comparison is bottleneck)
- **`resources/tests/sanity_tester.py`** â† Optimize regex

### Phase 3C (If overhead is bottleneck)
- **`resources/tests/sanity_tester.py`** â† Optimize process management

---

## ğŸ§ª Testing & Validation

### Before Optimization
```bash
# Configure & build
cmake -B build -S . -DCMAKE_BUILD_TYPE=Debug
cmake --build build --config Debug

# Run instrumented tests (baseline)
python3 resources/tests/sanity_tester.py \
    -d shows \
    -c ./build/tools/calchart_cmd/calchart_cmd
```

**Save**: Total time and metrics breakdown

### After Phase 3X Implementation
```bash
# Rebuild with changes
cmake --build build --config Debug

# Re-run instrumented tests
python3 resources/tests/sanity_tester.py \
    -d shows \
    -c ./build/tools/calchart_cmd/calchart_cmd
```

**Compare**: Before vs. after metrics. Look for speedup in the optimized phase.

---

## ğŸ’¡ Success Criteria

After completing Phase 1 + selected Phase 3X:

âœ“ **Debug mode**: 2-3x faster (example: 60s â†’ 20-30s)
âœ“ **Release mode**: 1.5-2x faster
âœ“ **All tests pass** with same accuracy
âœ“ **No regressions** in validation results
âœ“ **Metrics remain visible** for future optimization work

---

## ğŸ“ Questions & Troubleshooting

### Q: Where do I start?
**A**: Read [QUICK_REFERENCE.md](QUICK_REFERENCE.md), then [PHASE_1_IMPLEMENTATION.md](PHASE_1_IMPLEMENTATION.md)

### Q: How long will this take?
**A**: 
- Phase 1: 1-2 hours (instrumentation)
- Phase 2: 30 minutes (analysis)
- Phase 3X: 30 minutes - 4 hours (depends on bottleneck)
- **Total: 2-7 hours** depending on findings

### Q: Should I do all phases?
**A**: No. Do Phase 1, measure, do Phase 2, pick one Phase 3, optimize, measure again. Only add more phases if still too slow.

### Q: What if I'm wrong about the bottleneck?
**A**: Phase 1 tells you exactly. Trust the metrics, not guesses.

### Q: Can I test locally without full test suite?
**A**: Yes. Use a subset of show files with `python3 resources/tests/sanity_tester.py -d <your_test_dir>`

---

## ğŸ“ Complete Document Map

```
/private/tmp/dev/calchart.git/
â”œâ”€â”€ QUICK_REFERENCE.md ......................... Start here (5 min)
â”œâ”€â”€ PERFORMANCE_OPTIMIZATION_SUMMARY.md ....... Executive overview (10 min)
â”œâ”€â”€ VISUAL_WORKFLOW.md ........................ Visual guides & diagrams (15 min)
â”œâ”€â”€ PERFORMANCE_IMPROVEMENT_PLAN.md ........... Detailed strategy (30 min)
â””â”€â”€ PHASE_1_IMPLEMENTATION.md ................. Step-by-step code (implement)

Plus existing project files:
â”œâ”€â”€ resources/tests/sanity_tester.py ......... File to modify
â”œâ”€â”€ tools/calchart_cmd/main.cpp .............. File to modify (Phase 3A)
â”œâ”€â”€ tools/calchart_cmd/calchart_cmd_parse.hpp  File to modify (Phase 3A)
â””â”€â”€ src/core/ ............................... Potentially modified (Phase 3A)
```

---

## ğŸ”„ Workflow Summary

1. **Understand** â†’ Read [QUICK_REFERENCE.md](QUICK_REFERENCE.md)
2. **Plan** â†’ Read [PERFORMANCE_OPTIMIZATION_SUMMARY.md](PERFORMANCE_OPTIMIZATION_SUMMARY.md)
3. **Visualize** â†’ Review [VISUAL_WORKFLOW.md](VISUAL_WORKFLOW.md) diagrams
4. **Implement Phase 1** â†’ Follow [PHASE_1_IMPLEMENTATION.md](PHASE_1_IMPLEMENTATION.md)
5. **Measure & Analyze** â†’ Run tests, read metrics, decide Phase 3X
6. **Deep Dive** â†’ Read [PERFORMANCE_IMPROVEMENT_PLAN.md](PERFORMANCE_IMPROVEMENT_PLAN.md) Phase 3X section
7. **Implement Phase 3X** â†’ Code changes based on plan
8. **Validate** â†’ Re-measure, confirm speedup

---

## ğŸ“ Key Principles

1. **Measure before optimizing** â€” Avoid wasting effort on non-bottlenecks
2. **One phase at a time** â€” Understand impact of each change
3. **Keep instrumentation** â€” Metrics are valuable for future work
4. **Data-driven decisions** â€” Trust Phase 1 output, not guesses
5. **Iterative improvement** â€” Phase 1 + 3A is better than Phase 1 + 3A + 3B + 3C if 3A solves it

---

## âœ… Next Steps

**Right now:**
1. Read [QUICK_REFERENCE.md](QUICK_REFERENCE.md) (5 minutes)

**Then:**
2. Read [PHASE_1_IMPLEMENTATION.md](PHASE_1_IMPLEMENTATION.md) (10 minutes)

**Then:**
3. Implement Phase 1 instrumentation in `resources/tests/sanity_tester.py` (1-2 hours)

**Then:**
4. Run tests and analyze metrics (30 minutes)

**Then:**
5. Implement Phase 3X based on findings (30 min - 4 hours)

---

**Questions?** Refer to the specific guide sections above. This is a complete, self-contained optimization plan with all the information needed to succeed.

Good luck! ğŸš€

